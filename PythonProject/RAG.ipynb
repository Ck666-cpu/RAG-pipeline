{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RAG Testing",
   "id": "e3488fc71750432d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-16T04:30:12.669231Z",
     "start_time": "2025-09-16T04:29:52.056379Z"
    }
   },
   "source": [
    "!pip install -q langchain\n",
    "!pip install -q torch\n",
    "!pip install -q transformers\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q datasets\n",
    "!pip install -q faiss-cpu\n",
    "!pip install ipywidgets\n",
    "!pip install langchain-huggingface"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (9.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-huggingface in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from langchain-huggingface) (0.3.72)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from langchain-huggingface) (0.21.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.4 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from langchain-huggingface) (0.34.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.12.2)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.33.4->langchain-huggingface) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\jeff lam\\pycharmprojects\\pythonproject\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\jeff lam\\appdata\\roaming\\python\\python312\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T04:30:51.565308Z",
     "start_time": "2025-09-16T04:30:12.686213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ],
   "id": "ee9c9c42b2188a31",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T04:30:59.444439Z",
     "start_time": "2025-09-16T04:30:51.579697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify the dataset name and the column containing the content\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "page_content_column = \"context\"  # or any other column you're interested in\n",
    "\n",
    "# Create a loader instance\n",
    "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)\n",
    "\n",
    "# Load the data\n",
    "data = loader.load()\n",
    "\n",
    "# Display the first 15 entries\n",
    "data[:2]"
   ],
   "id": "bcb6295a0c25705a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'instruction': 'When did Virgin Australia start operating?', 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}, page_content='\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia\\'s domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"'),\n",
       " Document(metadata={'instruction': 'Which is a species of fish? Tope or Rope', 'response': 'Tope', 'category': 'classification'}, page_content='\"\"')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T04:31:00.671138Z",
     "start_time": "2025-09-16T04:30:59.467601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create an instance of the RecursiveCharacterTextSplitter class with specific parameters.\n",
    "# It splits text into chunks of 1000 characters each with a 150-character overlap.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "\n",
    "# 'data' holds the text you want to split, split the text into documents using the text splitter.\n",
    "docs = text_splitter.split_documents(data)"
   ],
   "id": "e8c5a2a2f8a459d8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T04:31:00.688866Z",
     "start_time": "2025-09-16T04:31:00.677289Z"
    }
   },
   "cell_type": "code",
   "source": "docs[0]",
   "id": "50af899dd916c899",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'instruction': 'When did Virgin Australia start operating?', 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}, page_content='\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia\\'s domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# load the model for embedding the texts",
   "id": "f24f8f0a508677d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T04:31:14.883889Z",
     "start_time": "2025-09-16T04:31:00.703525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the path to the pre-trained model you want to use\n",
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cpu'}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ],
   "id": "cf09bc674cd636d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jeff Lam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T04:31:15.065132Z",
     "start_time": "2025-09-16T04:31:14.893692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "query_result[:3]"
   ],
   "id": "c46347355e0afa0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.03833850845694542, 0.1234646737575531, -0.028642946854233742]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# storing vectors into a FAISS Vector database\n",
    "-it embeds each of the chunks during the storing"
   ],
   "id": "a901e9c978da80be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T04:34:21.310726Z",
     "start_time": "2025-09-16T04:31:15.075083Z"
    }
   },
   "cell_type": "code",
   "source": "db = FAISS.from_documents(docs, embeddings)",
   "id": "ee69b5e9c5165385",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T04:34:21.474749Z",
     "start_time": "2025-09-16T04:34:21.347085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"What is cheesemaking?\"\n",
    "searchDocs = db.similarity_search(question)\n",
    "print(searchDocs[0].page_content)"
   ],
   "id": "f1b2c90d192a238c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The goal of cheese making is to control the spoiling of milk into cheese. The milk is traditionally from a cow, goat, sheep or buffalo, although, in theory, cheese could be made from the milk of any mammal. Cow's milk is most commonly used worldwide. The cheesemaker's goal is a consistent product with specific characteristics (appearance, aroma, taste, texture). The process used to make a Camembert will be similar to, but not quite the same as, that used to make Cheddar.\\n\\nSome cheeses may be deliberately left to ferment from naturally airborne spores and bacteria; this approach generally leads to a less consistent product but one that is valuable in a niche market.\\n\\nCulturing\\nCheese is made by bringing milk (possibly pasteurised) in the cheese vat to a temperature required to promote the growth of the bacteria that feed on lactose and thus ferment the lactose into lactic acid. These bacteria in the milk may be wild, as is the case with unpasteurised milk, added from a culture,\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# load the fine_tuned model for tokenize the query",
   "id": "e0c76bad28a0543c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T04:34:23.155323Z",
     "start_time": "2025-09-16T04:34:21.487685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a tokenizer object by loading the pretrained \"Intel/dynamic_tinybert\" tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Intel/dynamic_tinybert\")\n",
    "\n",
    "# Create a question-answering model object by loading the pretrained \"Intel/dynamic_tinybert\" model.\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"Intel/dynamic_tinybert\")"
   ],
   "id": "69fcc7f623aa7663",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T04:34:25.000184Z",
     "start_time": "2025-09-16T04:34:23.165098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify the model name you want to use\n",
    "model_name = \"Intel/dynamic_tinybert\"\n",
    "\n",
    "# Load the tokenizer associated with the specified model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Define a question-answering pipeline using the model and tokenizer\n",
    "question_answerer = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
    "# with additional model-specific arguments (temperature and max_length)\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=question_answerer,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")"
   ],
   "id": "654c07b785b6ca6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T04:35:15.323092Z",
     "start_time": "2025-09-16T04:34:25.017139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model = 'HuggingFaceH4/zephyr-7b-beta')\n",
    "generator(\"Hello, I'm a language model\", max_length = 30, num_return_sequences=3)"
   ],
   "id": "86d6ccd19f8aa907",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14ec45538afb44649a33ac01ae1a88de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Greedy methods without beam search do not support `num_return_sequences` different than 1 (got 3).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m pipeline\n\u001B[32m      2\u001B[39m generator = pipeline(\u001B[33m'\u001B[39m\u001B[33mtext-generation\u001B[39m\u001B[33m'\u001B[39m, model = \u001B[33m'\u001B[39m\u001B[33mHuggingFaceH4/zephyr-7b-beta\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[43mgenerator\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mHello, I\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mm a language model\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_return_sequences\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\text_generation.py:285\u001B[39m, in \u001B[36mTextGenerationPipeline.__call__\u001B[39m\u001B[34m(self, text_inputs, **kwargs)\u001B[39m\n\u001B[32m    283\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    284\u001B[39m                 \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m(\u001B[38;5;28mlist\u001B[39m(chats), **kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m285\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtext_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\base.py:1362\u001B[39m, in \u001B[36mPipeline.__call__\u001B[39m\u001B[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[39m\n\u001B[32m   1354\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[32m   1355\u001B[39m         \u001B[38;5;28miter\u001B[39m(\n\u001B[32m   1356\u001B[39m             \u001B[38;5;28mself\u001B[39m.get_iterator(\n\u001B[32m   (...)\u001B[39m\u001B[32m   1359\u001B[39m         )\n\u001B[32m   1360\u001B[39m     )\n\u001B[32m   1361\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1362\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\base.py:1369\u001B[39m, in \u001B[36mPipeline.run_single\u001B[39m\u001B[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[39m\n\u001B[32m   1367\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[32m   1368\u001B[39m     model_inputs = \u001B[38;5;28mself\u001B[39m.preprocess(inputs, **preprocess_params)\n\u001B[32m-> \u001B[39m\u001B[32m1369\u001B[39m     model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1370\u001B[39m     outputs = \u001B[38;5;28mself\u001B[39m.postprocess(model_outputs, **postprocess_params)\n\u001B[32m   1371\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\base.py:1269\u001B[39m, in \u001B[36mPipeline.forward\u001B[39m\u001B[34m(self, model_inputs, **forward_params)\u001B[39m\n\u001B[32m   1267\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[32m   1268\u001B[39m         model_inputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_inputs, device=\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m-> \u001B[39m\u001B[32m1269\u001B[39m         model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1270\u001B[39m         model_outputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m   1271\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\text_generation.py:383\u001B[39m, in \u001B[36mTextGenerationPipeline._forward\u001B[39m\u001B[34m(self, model_inputs, **generate_kwargs)\u001B[39m\n\u001B[32m    380\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mgeneration_config\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m generate_kwargs:\n\u001B[32m    381\u001B[39m     generate_kwargs[\u001B[33m\"\u001B[39m\u001B[33mgeneration_config\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28mself\u001B[39m.generation_config\n\u001B[32m--> \u001B[39m\u001B[32m383\u001B[39m generated_sequence = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    384\u001B[39m out_b = generated_sequence.shape[\u001B[32m0\u001B[39m]\n\u001B[32m    385\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.framework == \u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\utils.py:2011\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[39m\n\u001B[32m   2008\u001B[39m tokenizer = kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mtokenizer\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)  \u001B[38;5;66;03m# Pull this out first, we only use it for stopping criteria\u001B[39;00m\n\u001B[32m   2009\u001B[39m assistant_tokenizer = kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33massistant_tokenizer\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)  \u001B[38;5;66;03m# only used for assisted generation\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2011\u001B[39m generation_config, model_kwargs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_prepare_generation_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2012\u001B[39m \u001B[38;5;28mself\u001B[39m._validate_model_kwargs(model_kwargs.copy())\n\u001B[32m   2013\u001B[39m \u001B[38;5;28mself\u001B[39m._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\utils.py:1545\u001B[39m, in \u001B[36mGenerationMixin._prepare_generation_config\u001B[39m\u001B[34m(self, generation_config, **kwargs)\u001B[39m\n\u001B[32m   1543\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[32m   1544\u001B[39m     generation_config = copy.deepcopy(generation_config)\n\u001B[32m-> \u001B[39m\u001B[32m1545\u001B[39m     model_kwargs = \u001B[43mgeneration_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1546\u001B[39m     \u001B[38;5;66;03m# If `generation_config` is provided, let's fallback ALL special tokens to the default values for the model\u001B[39;00m\n\u001B[32m   1547\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m using_model_generation_config:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:1325\u001B[39m, in \u001B[36mGenerationConfig.update\u001B[39m\u001B[34m(self, **kwargs)\u001B[39m\n\u001B[32m   1322\u001B[39m         to_remove.append(key)\n\u001B[32m   1324\u001B[39m \u001B[38;5;66;03m# Confirm that the updated instance is still valid\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1325\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mvalidate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1327\u001B[39m \u001B[38;5;66;03m# Remove all the attributes that were updated, without modifying the input dict\u001B[39;00m\n\u001B[32m   1328\u001B[39m unused_kwargs = {key: value \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m kwargs.items() \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m to_remove}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:752\u001B[39m, in \u001B[36mGenerationConfig.validate\u001B[39m\u001B[34m(self, is_init)\u001B[39m\n\u001B[32m    750\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.num_beams == \u001B[32m1\u001B[39m:\n\u001B[32m    751\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.do_sample \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m752\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    753\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mGreedy methods without beam search do not support `num_return_sequences` different than 1 \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    754\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m(got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.num_return_sequences\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m).\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    755\u001B[39m         )\n\u001B[32m    756\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.num_return_sequences > \u001B[38;5;28mself\u001B[39m.num_beams:\n\u001B[32m    757\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    758\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m`num_return_sequences` (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.num_return_sequences\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) has to be smaller or equal to `num_beams` \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    759\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.num_beams\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m).\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    760\u001B[39m     )\n",
      "\u001B[31mValueError\u001B[39m: Greedy methods without beam search do not support `num_return_sequences` different than 1 (got 3)."
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T07:41:37.538727Z",
     "start_time": "2025-09-16T07:41:37.522132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a retriever object from the 'db' using the 'as_retriever' method.\n",
    "# This retriever is likely used for retrieving data or documents from the database.\n",
    "retriever = db.as_retriever()"
   ],
   "id": "225ffc9a51b67422",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T07:41:40.759259Z",
     "start_time": "2025-09-16T07:41:40.240672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "docs = retriever.invoke(\"What is Cheesemaking?\")\n",
    "print(docs[0].page_content)"
   ],
   "id": "e4cc0f645d783aa6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The goal of cheese making is to control the spoiling of milk into cheese. The milk is traditionally from a cow, goat, sheep or buffalo, although, in theory, cheese could be made from the milk of any mammal. Cow's milk is most commonly used worldwide. The cheesemaker's goal is a consistent product with specific characteristics (appearance, aroma, taste, texture). The process used to make a Camembert will be similar to, but not quite the same as, that used to make Cheddar.\\n\\nSome cheeses may be deliberately left to ferment from naturally airborne spores and bacteria; this approach generally leads to a less consistent product but one that is valuable in a niche market.\\n\\nCulturing\\nCheese is made by bringing milk (possibly pasteurised) in the cheese vat to a temperature required to promote the growth of the bacteria that feed on lactose and thus ferment the lactose into lactic acid. These bacteria in the milk may be wild, as is the case with unpasteurised milk, added from a culture,\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T07:41:44.145465Z",
     "start_time": "2025-09-16T07:41:44.124597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a retriever object from the 'db' with a search configuration where it retrieves up to 4 relevant splits/documents.\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Create a question-answering instance (qa) using the RetrievalQA class.\n",
    "# It's configured with a language model (llm), a chain type \"refine,\" the retriever we created, and an option to not return source documents.\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever, return_source_documents=False)"
   ],
   "id": "d4dbca00ca837c7d",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T07:41:57.331472Z",
     "start_time": "2025-09-16T07:41:57.325828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def err_remove(er):\n",
    " lin = \"------------\"\n",
    " er = str(er)\n",
    " start_index = er.find(lin) + len(lin)\n",
    " end_index = er.rfind(lin)\n",
    " ans = er[start_index:end_index].strip()\n",
    " return ans"
   ],
   "id": "6cbf212178614b0e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T07:42:29.796256Z",
     "start_time": "2025-09-16T07:42:29.179958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"Who is Hamlet ?\"\n",
    "result = qa.invoke({\"query\": question})\n",
    "result[\"result\"]\n",
    "\n"
   ],
   "id": "477e17a64aa7835e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeff Lam\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Context information is below. \n------------\n\"Shakespeare produced most of his known works between 1589 and 1613. His early plays were primarily comedies and histories and are regarded as some of the best works produced in these genres. He then wrote mainly tragedies until 1608, among them Hamlet, Romeo and Juliet, Othello, King Lear, and Macbeth, all considered to be among the finest works in the English language. In the last phase of his life, he wrote tragicomedies (also known as romances) and collaborated with other playwrights.\"\n------------\nGiven the context information and not prior knowledge, answer the question: Who is Hamlet ?\n argument needs to be of type (SquadExample, dict)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m question = \u001B[33m\"\u001B[39m\u001B[33mWho is Hamlet ?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m result = \u001B[43mqa\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mquery\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m result[\u001B[33m\"\u001B[39m\u001B[33mresult\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:165\u001B[39m, in \u001B[36mChain.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    162\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    163\u001B[39m     \u001B[38;5;28mself\u001B[39m._validate_inputs(inputs)\n\u001B[32m    164\u001B[39m     outputs = (\n\u001B[32m--> \u001B[39m\u001B[32m165\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    166\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m    167\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call(inputs)\n\u001B[32m    168\u001B[39m     )\n\u001B[32m    170\u001B[39m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] = \u001B[38;5;28mself\u001B[39m.prep_outputs(\n\u001B[32m    171\u001B[39m         inputs,\n\u001B[32m    172\u001B[39m         outputs,\n\u001B[32m    173\u001B[39m         return_only_outputs,\n\u001B[32m    174\u001B[39m     )\n\u001B[32m    175\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:159\u001B[39m, in \u001B[36mBaseRetrievalQA._call\u001B[39m\u001B[34m(self, inputs, run_manager)\u001B[39m\n\u001B[32m    157\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    158\u001B[39m     docs = \u001B[38;5;28mself\u001B[39m._get_docs(question)  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m159\u001B[39m answer = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcombine_documents_chain\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    160\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_documents\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m=\u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    162\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_run_manager\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    163\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    165\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_source_documents:\n\u001B[32m    166\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;28mself\u001B[39m.output_key: answer, \u001B[33m\"\u001B[39m\u001B[33msource_documents\u001B[39m\u001B[33m\"\u001B[39m: docs}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001B[39m, in \u001B[36mwarning_emitting_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    179\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwarning_emitting_wrapper\u001B[39m(*args: Any, **kwargs: Any) -> Any:\n\u001B[32m    180\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Wrapper for the original wrapped callable that emits a warning.\u001B[39;00m\n\u001B[32m    181\u001B[39m \n\u001B[32m    182\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    187\u001B[39m \u001B[33;03m        The return value of the function being wrapped.\u001B[39;00m\n\u001B[32m    188\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m     \u001B[38;5;28;01mnonlocal\u001B[39;00m warned\n\u001B[32m    190\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m warned \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_caller_internal():\n\u001B[32m    191\u001B[39m         warned = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:632\u001B[39m, in \u001B[36mChain.run\u001B[39m\u001B[34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[39m\n\u001B[32m    627\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(args[\u001B[32m0\u001B[39m], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001B[32m    628\u001B[39m         _output_key\n\u001B[32m    629\u001B[39m     ]\n\u001B[32m    631\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[32m--> \u001B[39m\u001B[32m632\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m[\n\u001B[32m    633\u001B[39m         _output_key\n\u001B[32m    634\u001B[39m     ]\n\u001B[32m    636\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[32m    637\u001B[39m     msg = (\n\u001B[32m    638\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m`run` supported with either positional arguments or keyword arguments,\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    639\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m but none were provided.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    640\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001B[39m, in \u001B[36mwarning_emitting_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    179\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwarning_emitting_wrapper\u001B[39m(*args: Any, **kwargs: Any) -> Any:\n\u001B[32m    180\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Wrapper for the original wrapped callable that emits a warning.\u001B[39;00m\n\u001B[32m    181\u001B[39m \n\u001B[32m    182\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    187\u001B[39m \u001B[33;03m        The return value of the function being wrapped.\u001B[39;00m\n\u001B[32m    188\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m     \u001B[38;5;28;01mnonlocal\u001B[39;00m warned\n\u001B[32m    190\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m warned \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_caller_internal():\n\u001B[32m    191\u001B[39m         warned = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:410\u001B[39m, in \u001B[36mChain.__call__\u001B[39m\u001B[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[39m\n\u001B[32m    378\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[32m    379\u001B[39m \n\u001B[32m    380\u001B[39m \u001B[33;03mArgs:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    401\u001B[39m \u001B[33;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[32m    402\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    403\u001B[39m config = {\n\u001B[32m    404\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcallbacks\u001B[39m\u001B[33m\"\u001B[39m: callbacks,\n\u001B[32m    405\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtags\u001B[39m\u001B[33m\"\u001B[39m: tags,\n\u001B[32m    406\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: metadata,\n\u001B[32m    407\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mrun_name\u001B[39m\u001B[33m\"\u001B[39m: run_name,\n\u001B[32m    408\u001B[39m }\n\u001B[32m--> \u001B[39m\u001B[32m410\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    411\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    412\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRunnableConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    413\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    414\u001B[39m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[43m=\u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    415\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:165\u001B[39m, in \u001B[36mChain.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    162\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    163\u001B[39m     \u001B[38;5;28mself\u001B[39m._validate_inputs(inputs)\n\u001B[32m    164\u001B[39m     outputs = (\n\u001B[32m--> \u001B[39m\u001B[32m165\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    166\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m    167\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call(inputs)\n\u001B[32m    168\u001B[39m     )\n\u001B[32m    170\u001B[39m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] = \u001B[38;5;28mself\u001B[39m.prep_outputs(\n\u001B[32m    171\u001B[39m         inputs,\n\u001B[32m    172\u001B[39m         outputs,\n\u001B[32m    173\u001B[39m         return_only_outputs,\n\u001B[32m    174\u001B[39m     )\n\u001B[32m    175\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:143\u001B[39m, in \u001B[36mBaseCombineDocumentsChain._call\u001B[39m\u001B[34m(self, inputs, run_manager)\u001B[39m\n\u001B[32m    141\u001B[39m \u001B[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001B[39;00m\n\u001B[32m    142\u001B[39m other_keys = {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m inputs.items() \u001B[38;5;28;01mif\u001B[39;00m k != \u001B[38;5;28mself\u001B[39m.input_key}\n\u001B[32m--> \u001B[39m\u001B[32m143\u001B[39m output, extra_return_dict = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcombine_docs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    144\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    145\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_run_manager\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    146\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mother_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    147\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    148\u001B[39m extra_return_dict[\u001B[38;5;28mself\u001B[39m.output_key] = output\n\u001B[32m    149\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m extra_return_dict\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain\\chains\\combine_documents\\refine.py:170\u001B[39m, in \u001B[36mRefineDocumentsChain.combine_docs\u001B[39m\u001B[34m(self, docs, callbacks, **kwargs)\u001B[39m\n\u001B[32m    157\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Combine by mapping first chain over all, then stuffing into final chain.\u001B[39;00m\n\u001B[32m    158\u001B[39m \n\u001B[32m    159\u001B[39m \u001B[33;03mArgs:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    167\u001B[39m \u001B[33;03m    element returned is a dictionary of other keys to return.\u001B[39;00m\n\u001B[32m    168\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    169\u001B[39m inputs = \u001B[38;5;28mself\u001B[39m._construct_initial_inputs(docs, **kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m170\u001B[39m res = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minitial_llm_chain\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    171\u001B[39m refine_steps = [res]\n\u001B[32m    172\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m docs[\u001B[32m1\u001B[39m:]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py:325\u001B[39m, in \u001B[36mLLMChain.predict\u001B[39m\u001B[34m(self, callbacks, **kwargs)\u001B[39m\n\u001B[32m    310\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, callbacks: Callbacks = \u001B[38;5;28;01mNone\u001B[39;00m, **kwargs: Any) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m    311\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001B[39;00m\n\u001B[32m    312\u001B[39m \n\u001B[32m    313\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    323\u001B[39m \u001B[33;03m            completion = llm.predict(adjective=\"funny\")\u001B[39;00m\n\u001B[32m    324\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m325\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;28mself\u001B[39m.output_key]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001B[39m, in \u001B[36mwarning_emitting_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    179\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwarning_emitting_wrapper\u001B[39m(*args: Any, **kwargs: Any) -> Any:\n\u001B[32m    180\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Wrapper for the original wrapped callable that emits a warning.\u001B[39;00m\n\u001B[32m    181\u001B[39m \n\u001B[32m    182\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    187\u001B[39m \u001B[33;03m        The return value of the function being wrapped.\u001B[39;00m\n\u001B[32m    188\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m     \u001B[38;5;28;01mnonlocal\u001B[39;00m warned\n\u001B[32m    190\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m warned \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_caller_internal():\n\u001B[32m    191\u001B[39m         warned = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:410\u001B[39m, in \u001B[36mChain.__call__\u001B[39m\u001B[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[39m\n\u001B[32m    378\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[32m    379\u001B[39m \n\u001B[32m    380\u001B[39m \u001B[33;03mArgs:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    401\u001B[39m \u001B[33;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[32m    402\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    403\u001B[39m config = {\n\u001B[32m    404\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcallbacks\u001B[39m\u001B[33m\"\u001B[39m: callbacks,\n\u001B[32m    405\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtags\u001B[39m\u001B[33m\"\u001B[39m: tags,\n\u001B[32m    406\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: metadata,\n\u001B[32m    407\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mrun_name\u001B[39m\u001B[33m\"\u001B[39m: run_name,\n\u001B[32m    408\u001B[39m }\n\u001B[32m--> \u001B[39m\u001B[32m410\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    411\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    412\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRunnableConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    413\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    414\u001B[39m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[43m=\u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    415\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:165\u001B[39m, in \u001B[36mChain.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    162\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    163\u001B[39m     \u001B[38;5;28mself\u001B[39m._validate_inputs(inputs)\n\u001B[32m    164\u001B[39m     outputs = (\n\u001B[32m--> \u001B[39m\u001B[32m165\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    166\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m    167\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call(inputs)\n\u001B[32m    168\u001B[39m     )\n\u001B[32m    170\u001B[39m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] = \u001B[38;5;28mself\u001B[39m.prep_outputs(\n\u001B[32m    171\u001B[39m         inputs,\n\u001B[32m    172\u001B[39m         outputs,\n\u001B[32m    173\u001B[39m         return_only_outputs,\n\u001B[32m    174\u001B[39m     )\n\u001B[32m    175\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py:127\u001B[39m, in \u001B[36mLLMChain._call\u001B[39m\u001B[34m(self, inputs, run_manager)\u001B[39m\n\u001B[32m    122\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_call\u001B[39m(\n\u001B[32m    123\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    124\u001B[39m     inputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m    125\u001B[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    126\u001B[39m ) -> \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m127\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    128\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.create_outputs(response)[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py:139\u001B[39m, in \u001B[36mLLMChain.generate\u001B[39m\u001B[34m(self, input_list, run_manager)\u001B[39m\n\u001B[32m    137\u001B[39m callbacks = run_manager.get_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    138\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m.llm, BaseLanguageModel):\n\u001B[32m--> \u001B[39m\u001B[32m139\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mllm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    140\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    141\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    142\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    143\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mllm_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    144\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    145\u001B[39m results = \u001B[38;5;28mself\u001B[39m.llm.bind(stop=stop, **\u001B[38;5;28mself\u001B[39m.llm_kwargs).batch(\n\u001B[32m    146\u001B[39m     cast(\u001B[38;5;28mlist\u001B[39m, prompts),\n\u001B[32m    147\u001B[39m     {\u001B[33m\"\u001B[39m\u001B[33mcallbacks\u001B[39m\u001B[33m\"\u001B[39m: callbacks},\n\u001B[32m    148\u001B[39m )\n\u001B[32m    149\u001B[39m generations: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[Generation]] = []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:766\u001B[39m, in \u001B[36mgenerate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m    736\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_astream\u001B[39m(\n\u001B[32m    737\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    738\u001B[39m     prompt: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    741\u001B[39m     **kwargs: Any,\n\u001B[32m    742\u001B[39m ) -> AsyncIterator[GenerationChunk]:\n\u001B[32m    743\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"An async version of the _stream method.\u001B[39;00m\n\u001B[32m    744\u001B[39m \n\u001B[32m    745\u001B[39m \u001B[33;03m    The default implementation uses the synchronous _stream method and wraps it in\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    758\u001B[39m \u001B[33;03m        Generation chunks.\u001B[39;00m\n\u001B[32m    759\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m    760\u001B[39m     iterator = \u001B[38;5;28;01mawait\u001B[39;00m run_in_executor(\n\u001B[32m    761\u001B[39m         \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    762\u001B[39m         \u001B[38;5;28mself\u001B[39m._stream,\n\u001B[32m    763\u001B[39m         prompt,\n\u001B[32m    764\u001B[39m         stop,\n\u001B[32m    765\u001B[39m         run_manager.get_sync() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m--> \u001B[39m\u001B[32m766\u001B[39m         **kwargs,\n\u001B[32m    767\u001B[39m     )\n\u001B[32m    768\u001B[39m     done = \u001B[38;5;28mobject\u001B[39m()\n\u001B[32m    769\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:971\u001B[39m, in \u001B[36mgenerate\u001B[39m\u001B[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    958\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    959\u001B[39m     \u001B[38;5;66;03m# We've received a single callbacks arg to apply to all inputs\u001B[39;00m\n\u001B[32m    960\u001B[39m     callback_managers = [\n\u001B[32m    961\u001B[39m         CallbackManager.configure(\n\u001B[32m    962\u001B[39m             cast(\u001B[33m\"\u001B[39m\u001B[33mCallbacks\u001B[39m\u001B[33m\"\u001B[39m, callbacks),\n\u001B[32m   (...)\u001B[39m\u001B[32m    969\u001B[39m         )\n\u001B[32m    970\u001B[39m     ] * \u001B[38;5;28mlen\u001B[39m(prompts)\n\u001B[32m--> \u001B[39m\u001B[32m971\u001B[39m     run_name_list = [cast(\u001B[33m\"\u001B[39m\u001B[33mOptional[str]\u001B[39m\u001B[33m\"\u001B[39m, run_name)] * \u001B[38;5;28mlen\u001B[39m(prompts)\n\u001B[32m    972\u001B[39m run_ids_list = \u001B[38;5;28mself\u001B[39m._get_run_ids_list(run_id, prompts)\n\u001B[32m    973\u001B[39m params = \u001B[38;5;28mself\u001B[39m.dict()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001B[39m, in \u001B[36m_generate_helper\u001B[39m\u001B[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[39m\n\u001B[32m    788\u001B[39m     prompt_strings = [p.to_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m    789\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n\u001B[32m    791\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m792\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34magenerate_prompt\u001B[39m(\n\u001B[32m    793\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    794\u001B[39m     prompts: \u001B[38;5;28mlist\u001B[39m[PromptValue],\n\u001B[32m    795\u001B[39m     stop: Optional[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    796\u001B[39m     callbacks: Optional[Union[Callbacks, \u001B[38;5;28mlist\u001B[39m[Callbacks]]] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    797\u001B[39m     **kwargs: Any,\n\u001B[32m    798\u001B[39m ) -> LLMResult:\n\u001B[32m    799\u001B[39m     prompt_strings = [p.to_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m    800\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.agenerate(\n\u001B[32m    801\u001B[39m         prompt_strings, stop=stop, callbacks=callbacks, **kwargs\n\u001B[32m    802\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\langchain_huggingface\\llms\\huggingface_pipeline.py:323\u001B[39m, in \u001B[36mHuggingFacePipeline._generate\u001B[39m\u001B[34m(self, prompts, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    320\u001B[39m batch_prompts = prompts[i : i + \u001B[38;5;28mself\u001B[39m.batch_size]\n\u001B[32m    322\u001B[39m \u001B[38;5;66;03m# Process batch of prompts\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m323\u001B[39m responses = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    324\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_prompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    325\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mpipeline_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    326\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    328\u001B[39m \u001B[38;5;66;03m# Process each response in the batch\u001B[39;00m\n\u001B[32m    329\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m j, response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(responses):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\question_answering.py:396\u001B[39m, in \u001B[36mQuestionAnsweringPipeline.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    390\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m args:\n\u001B[32m    391\u001B[39m     warnings.warn(\n\u001B[32m    392\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mPassing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    393\u001B[39m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[32m    394\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m396\u001B[39m examples = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_args_parser\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    397\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(examples, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(examples) == \u001B[32m1\u001B[39m:\n\u001B[32m    398\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m(examples[\u001B[32m0\u001B[39m], **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\question_answering.py:227\u001B[39m, in \u001B[36mQuestionAnsweringArgumentHandler.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    224\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mInvalid arguments \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkwargs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    226\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, item \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(inputs):\n\u001B[32m--> \u001B[39m\u001B[32m227\u001B[39m     inputs[i] = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnormalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    229\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m inputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\question_answering.py:172\u001B[39m, in \u001B[36mQuestionAnsweringArgumentHandler.normalize\u001B[39m\u001B[34m(self, item)\u001B[39m\n\u001B[32m    169\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m`\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m` cannot be empty\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    171\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QuestionAnsweringPipeline.create_sample(**item)\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mitem\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m argument needs to be of type (SquadExample, dict)\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mValueError\u001B[39m: Context information is below. \n------------\n\"Shakespeare produced most of his known works between 1589 and 1613. His early plays were primarily comedies and histories and are regarded as some of the best works produced in these genres. He then wrote mainly tragedies until 1608, among them Hamlet, Romeo and Juliet, Othello, King Lear, and Macbeth, all considered to be among the finest works in the English language. In the last phase of his life, he wrote tragicomedies (also known as romances) and collaborated with other playwrights.\"\n------------\nGiven the context information and not prior knowledge, answer the question: Who is Hamlet ?\n argument needs to be of type (SquadExample, dict)"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:37:48.046658Z",
     "start_time": "2025-10-15T07:07:54.065986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# 1. Set up a text2text-generation pipeline.\n",
    "#    We use device=-1 to ensure it runs on the CPU.\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    tokenizer=\"google/flan-t5-base\",\n",
    "    max_length=512,\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "# 2. Wrap it in the LangChain pipeline object\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 3. Re-create the RetrievalQA chain with the new, correct LLM.\n",
    "#    The 'retriever' object is already correctly defined from the previous cells.\n",
    "#    We'll use the 'stuff' chain type, which is standard and efficient. It \"stuffs\" all\n",
    "#    retrieved documents into the prompt for the LLM.\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False\n",
    ")\n",
    "\n",
    "# 4. Ask the original question again. This will now work correctly.\n",
    "question = \"Who is Hamlet?\"\n",
    "result = qa.invoke({\"query\": question})\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {result['result']}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 5. Let's try another question to confirm the RAG pipeline is working.\n",
    "question_2 = \"When did Virgin Australia start operating?\"\n",
    "result_2 = qa.invoke({\"query\": question_2})\n",
    "print(f\"Question: {question_2}\")\n",
    "print(f\"Answer: {result_2['result']}\")"
   ],
   "id": "9e1050833837c7a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "258a7d2789a54c9ab92bb69de05f73f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeff Lam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jeff Lam\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jeff Lam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a423935dd264d53926ade568319cf00"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/google/flan-t5-base/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:  43%|####3     | 430M/990M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ed5ab2d6b864dd682e7acddfcbff384"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/google/flan-t5-base/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:  43%|####3     | 430M/990M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e070e85e7f9d4c5f8d2889dba9031eee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/google/flan-t5-base/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:  43%|####3     | 430M/990M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fbc3fbcd310643c196b3dabfc19cef5f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/google/flan-t5-base/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:  43%|####3     | 430M/990M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27035bedc1d64a1b9289e315fe2e64f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jeff Lam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeff Lam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tf_keras\\src\\initializers\\initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e39f88860ddb4545a3b4771588501027"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bfa5e57fdd174a8b9eb30533b26035fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80c9a74f41d0435ea5984c1784836073"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7419a9449b8e45d9a8757789b6603917"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use -1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RetrievalQA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 21\u001B[39m\n\u001B[32m     15\u001B[39m llm = HuggingFacePipeline(pipeline=pipe)\n\u001B[32m     17\u001B[39m \u001B[38;5;66;03m# 3. Re-create the RetrievalQA chain with the new, correct LLM.\u001B[39;00m\n\u001B[32m     18\u001B[39m \u001B[38;5;66;03m#    The 'retriever' object is already correctly defined from the previous cells.\u001B[39;00m\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m#    We'll use the 'stuff' chain type, which is standard and efficient. It \"stuffs\" all\u001B[39;00m\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m#    retrieved documents into the prompt for the LLM.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m qa = \u001B[43mRetrievalQA\u001B[49m.from_chain_type(\n\u001B[32m     22\u001B[39m     llm=llm,\n\u001B[32m     23\u001B[39m     chain_type=\u001B[33m\"\u001B[39m\u001B[33mstuff\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     24\u001B[39m     retriever=retriever,\n\u001B[32m     25\u001B[39m     return_source_documents=\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m     26\u001B[39m )\n\u001B[32m     28\u001B[39m \u001B[38;5;66;03m# 4. Ask the original question again. This will now work correctly.\u001B[39;00m\n\u001B[32m     29\u001B[39m question = \u001B[33m\"\u001B[39m\u001B[33mWho is Hamlet?\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mNameError\u001B[39m: name 'RetrievalQA' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:42:20.170400Z",
     "start_time": "2025-10-15T07:42:15.578176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 1. INSTALLATION\n",
    "# ==============================================================================\n",
    "# Install all the required libraries. The '-q' flag is for a quiet installation.\n",
    "!pip install -q langchain torch transformers sentence-transformers datasets faiss-cpu ipywidgets langchain-huggingface\n"
   ],
   "id": "d92b2a18a69af1be",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:42:27.794839Z",
     "start_time": "2025-10-15T07:42:27.462272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 2. IMPORTS\n",
    "# ==============================================================================\n",
    "# Core LangChain and data loading components\n",
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Hugging Face specific components\n",
    "from transformers import pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ],
   "id": "59f5876cfa443ada",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:42:53.519449Z",
     "start_time": "2025-10-15T07:42:39.342951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 3. DATA LOADING AND PREPARATION\n",
    "# ==============================================================================\n",
    "print(\"Step 3: Loading and preparing data...\")\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "page_content_column = \"context\"\n",
    "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)\n",
    "data = loader.load()\n",
    "\n",
    "# Split the loaded documents into smaller, manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "print(\"Data loaded and split into chunks.\")"
   ],
   "id": "3ba7356d0797f25b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Loading and preparing data...\n",
      "Data loaded and split into chunks.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:47:24.190050Z",
     "start_time": "2025-10-15T07:43:38.552221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 4. EMBEDDING MODEL AND VECTOR STORE\n",
    "# ==============================================================================\n",
    "print(\"\\nStep 4: Setting up embedding model and vector store...\")\n",
    "\n",
    "# Define the embedding model to convert text chunks into numerical vectors\n",
    "model_path = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_path,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Create the FAISS vector store to efficiently search the document embeddings\n",
    "# This process may take a few minutes as it embeds all document chunks\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "print(\"FAISS vector store created successfully.\")"
   ],
   "id": "d07e626ee813a23f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Setting up embedding model and vector store...\n",
      "FAISS vector store created successfully.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:47:52.361278Z",
     "start_time": "2025-10-15T07:47:50.230507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 5. RETRIEVER, LLM, AND RAG CHAIN SETUP\n",
    "# ==============================================================================\n",
    "print(\"\\nStep 5: Setting up the Retriever, LLM, and RAG chain...\")\n",
    "\n",
    "# Create a retriever from the vector store to fetch relevant documents\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Set up the Language Model (LLM) pipeline\n",
    "# We use a 'text2text-generation' model which is suitable for question-answering based on context\n",
    "# The 'device=-1' argument ensures the model runs on the CPU\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    tokenizer=\"google/flan-t5-base\",\n",
    "    max_length=512,\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Create the final Retrieval-Augmented Generation (RAG) chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # 'stuff' chain type passes all retrieved chunks to the LLM\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False\n",
    ")\n",
    "\n",
    "print(\"RAG chain is ready.\")"
   ],
   "id": "1ac54a3cbdeb4cc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Setting up the Retriever, LLM, and RAG chain...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "790fd92976564004a5bf0b631635c9c0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain is ready.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:48:07.943925Z",
     "start_time": "2025-10-15T07:48:02.510571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 6. EXECUTE QUERIES\n",
    "# ==============================================================================\n",
    "print(\"\\nStep 6: Executing queries...\")\n",
    "\n",
    "# --- Query 1 ---\n",
    "question_1 = \"Who is Hamlet?\"\n",
    "result_1 = qa_chain.invoke({\"query\": question_1})\n",
    "print(f\"\\nQuestion: {question_1}\")\n",
    "print(f\"Answer: {result_1['result']}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Query 2 ---\n",
    "question_2 = \"When did Virgin Australia start operating?\"\n",
    "result_2 = qa_chain.invoke({\"query\": question_2})\n",
    "print(f\"Question: {question_2}\")\n",
    "print(f\"Answer: {result_2['result']}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Query 3 ---\n",
    "question_3 = \"What is cheesemaking?\"\n",
    "result_3 = qa_chain.invoke({\"query\": question_3})\n",
    "print(f\"Question: {question_3}\")\n",
    "print(f\"Answer: {result_3['result']}\")\n",
    "print(\"-\" * 50)"
   ],
   "id": "71f6a0b23494240d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 6: Executing queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Who is Hamlet?\n",
      "Answer: Shakespeare\n",
      "--------------------------------------------------\n",
      "Question: When did Virgin Australia start operating?\n",
      "Answer: 31 August 2000\n",
      "--------------------------------------------------\n",
      "Question: What is cheesemaking?\n",
      "Answer: control the spoiling of milk into cheese\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1795e649913cf878"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
