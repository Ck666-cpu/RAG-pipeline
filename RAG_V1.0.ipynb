{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-15T07:51:36.952026Z",
     "start_time": "2025-10-15T07:51:34.376294Z"
    }
   },
   "source": [
    "# ==============================================================================\n",
    "# 1. INSTALLATION\n",
    "# ==============================================================================\n",
    "# Install all the required libraries. The '-q' flag is for a quiet installation.\n",
    "!pip install -q langchain torch transformers sentence-transformers datasets faiss-cpu ipywidgets langchain-huggingface\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:51:54.075507Z",
     "start_time": "2025-10-15T07:51:37.988149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 2. IMPORTS\n",
    "# ==============================================================================\n",
    "# Core LangChain and data loading components\n",
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Hugging Face specific components\n",
    "from transformers import pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ],
   "id": "72d30daed623303",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:52:02.880712Z",
     "start_time": "2025-10-15T07:51:54.081615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 3. DATA LOADING AND PREPARATION\n",
    "# ==============================================================================\n",
    "print(\"Step 3: Loading and preparing data...\")\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "page_content_column = \"context\"\n",
    "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)\n",
    "data = loader.load()\n",
    "\n",
    "# Split the loaded documents into smaller, manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "print(\"Data loaded and split into chunks.\")"
   ],
   "id": "e7fea24eed902adf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Loading and preparing data...\n",
      "Data loaded and split into chunks.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:56:37.208026Z",
     "start_time": "2025-10-15T07:52:02.919692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 4. EMBEDDING MODEL AND VECTOR STORE\n",
    "# ==============================================================================\n",
    "print(\"\\nStep 4: Setting up embedding model and vector store...\")\n",
    "\n",
    "# Define the embedding model to convert text chunks into numerical vectors\n",
    "model_path = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_path,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Create the FAISS vector store to efficiently search the document embeddings\n",
    "# This process may take a few minutes as it embeds all document chunks\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "print(\"FAISS vector store created successfully.\")"
   ],
   "id": "2ede3aab7a76cec3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Setting up embedding model and vector store...\n",
      "WARNING:tensorflow:From C:\\Users\\Jeff Lam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "FAISS vector store created successfully.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:56:39.787001Z",
     "start_time": "2025-10-15T07:56:37.282159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 5. RETRIEVER, LLM, AND RAG CHAIN SETUP\n",
    "# ==============================================================================\n",
    "print(\"\\nStep 5: Setting up the Retriever, LLM, and RAG chain...\")\n",
    "\n",
    "# Create a retriever from the vector store to fetch relevant documents\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Set up the Language Model (LLM) pipeline\n",
    "# We use a 'text2text-generation' model which is suitable for question-answering based on context\n",
    "# The 'device=-1' argument ensures the model runs on the CPU\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    tokenizer=\"google/flan-t5-base\",\n",
    "    max_length=512,\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Create the final Retrieval-Augmented Generation (RAG) chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # 'stuff' chain type passes all retrieved chunks to the LLM\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False\n",
    ")\n",
    "\n",
    "print(\"RAG chain is ready.\")"
   ],
   "id": "c2cdb7a5f33285b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Setting up the Retriever, LLM, and RAG chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain is ready.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:56:45.757667Z",
     "start_time": "2025-10-15T07:56:39.797998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 6. EXECUTE QUERIES\n",
    "# ==============================================================================\n",
    "print(\"\\nStep 6: Executing queries...\")\n",
    "\n",
    "# --- Query 1 ---\n",
    "question_1 = \"Who is Hamlet?\"\n",
    "result_1 = qa_chain.invoke({\"query\": question_1})\n",
    "print(f\"\\nQuestion: {question_1}\")\n",
    "print(f\"Answer: {result_1['result']}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Query 2 ---\n",
    "question_2 = \"When did Virgin Australia start operating?\"\n",
    "result_2 = qa_chain.invoke({\"query\": question_2})\n",
    "print(f\"Question: {question_2}\")\n",
    "print(f\"Answer: {result_2['result']}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Query 3 ---\n",
    "question_3 = \"What is cheesemaking?\"\n",
    "result_3 = qa_chain.invoke({\"query\": question_3})\n",
    "print(f\"Question: {question_3}\")\n",
    "print(f\"Answer: {result_3['result']}\")\n",
    "print(\"-\" * 50)"
   ],
   "id": "58a8ee57141cb4c3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 6: Executing queries...\n",
      "\n",
      "Question: Who is Hamlet?\n",
      "Answer: Shakespeare\n",
      "--------------------------------------------------\n",
      "Question: When did Virgin Australia start operating?\n",
      "Answer: 31 August 2000\n",
      "--------------------------------------------------\n",
      "Question: What is cheesemaking?\n",
      "Answer: control the spoiling of milk into cheese\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "99235744a1c0b72e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
